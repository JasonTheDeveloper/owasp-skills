# 05 Improper Output Handling

Identifier: LLM05:2025
Category: Output Safety

## Description

Improper Output Handling refers to insufficient validation, sanitization, and handling of
the outputs generated by large language models before they are passed downstream to other
components and systems. Since LLM-generated content can be controlled by prompt input, this
behavior is similar to providing users indirect access to additional functionality.

Successful exploitation can result in XSS and CSRF in web browsers, SSRF, privilege
escalation, or remote code execution on backend systems.

Conditions that increase impact include granting the LLM privileges beyond what is intended
for end users, vulnerability to indirect prompt injection attacks, third-party extensions
that do not adequately validate inputs, lack of proper output encoding for different
contexts, insufficient monitoring and logging of LLM outputs, and absence of rate limiting
or anomaly detection.

## Common examples

1. LLM output is entered directly into a system shell or similar function such as exec or
   eval, resulting in remote code execution.
2. JavaScript or Markdown generated by the LLM is returned to a user and interpreted by the
   browser, resulting in XSS.
3. LLM-generated SQL queries are executed without proper parameterization, leading to SQL
   injection.
4. LLM output is used to construct file paths without proper sanitization, resulting in path
   traversal vulnerabilities.
5. LLM-generated content is used in email templates without proper escaping, leading to
   phishing attacks.

## Attack scenarios

### Scenario A — Extension shutdown via unvalidated output
An application utilizes an LLM extension to generate responses for a chatbot. The LLM
directly passes its response without proper output validation to a privileged extension,
causing the extension to shut down for maintenance.

### Scenario B — Data exfiltration via summarizer
A user utilizes a website summarizer tool powered by an LLM. The website includes a prompt
injection instructing the LLM to capture sensitive content and encode it, then send it
without output validation to an attacker-controlled server.

### Scenario C — SQL execution via chat
An LLM allows users to craft SQL queries for a backend database through a chat feature.
A user requests a query to delete all database tables. Without scrutiny of the crafted
query, all tables are deleted.

### Scenario D — XSS via unsanitized content
A web app uses an LLM to generate content from user text prompts without output sanitization.
An attacker submits a crafted prompt causing the LLM to return an unsanitized JavaScript
payload, leading to XSS when rendered on a victim's browser.

## Prevention and mitigation

1. Treat the model as any other user, adopting a zero-trust approach, and apply proper input
   validation on responses coming from the model to backend functions.
2. Follow OWASP ASVS (Application Security Verification Standard) guidelines for effective
   input validation and sanitization.
3. Encode model output back to users to mitigate undesired code execution by JavaScript or
   Markdown.
4. Implement context-aware output encoding based on where the LLM output will be used (HTML
   encoding for web content, SQL escaping for database queries).
5. Use parameterized queries or prepared statements for all database operations involving
   LLM output.
6. Employ strict Content Security Policies (CSP) to mitigate XSS from LLM-generated content.
7. Implement robust logging and monitoring systems to detect unusual patterns in LLM outputs
   indicating exploitation attempts.
