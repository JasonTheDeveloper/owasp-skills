# 04 Membership Inference Attack

Identifier: ML04:2023
Category: Privacy

## Description

Membership inference attacks occur when an attacker determines whether a specific data record was
included in the model's training dataset. By querying the model and analyzing its predictions, the
attacker infers membership information, which exposes sensitive details about individuals whose
data was used for training. The attack exploits differences in the model's behavior on data it has
seen during training versus data it has not, often leveraging overfitting as the primary signal.
This is particularly concerning for models trained on sensitive data such as financial records,
medical histories, or personal information.

## Risk

- Loss of confidentiality and privacy of sensitive data used for training.
- Unreliable or incorrect model predictions when the model overfits to training data.
- Legal and regulatory compliance violations from exposure of membership information.
- Reputational damage to the organization operating the model.
- Attackers with access to the data and the model can exploit unsecured data transmission
  channels.

## Vulnerability checklist

- The model overfits to the training data, behaving differently on training versus unseen data.
- Training data is accessible through unsecured data transmission channels.
- Proper data access controls are not in place.
- Data validation and sanitization techniques are insufficient.
- Training data is not encrypted in transit or at rest.
- No differential privacy techniques are applied during training.
- The model's predictions are returned with full confidence scores without noise or rounding.
- Regular testing for overfitting and membership inference susceptibility is not performed.

## Prevention controls

1. Train machine learning models on randomized or shuffled data to make it more difficult for an
   attacker to determine whether a particular example was included in the training dataset.
2. Obfuscate the model's predictions by adding random noise or using differential privacy
   techniques to make it harder for an attacker to determine the model's training data.
3. Apply regularization techniques such as L1 or L2 regularization to prevent overfitting, which
   reduces the model's ability to distinguish training data from unseen data.
4. Reduce the size of the training dataset or remove redundant or highly correlated features to
   reduce the information an attacker can gain.
5. Regularly test and monitor the model's behavior for anomalies to detect when an attacker is
   attempting to gain access to sensitive information.
6. Implement proper data access controls, encryption, and secure data transmission channels.

## Example attack scenarios

### Scenario A â€” Inferring financial data from a machine learning model
A malicious attacker wants to gain access to sensitive financial information of individuals. They
train a machine learning model on a dataset of financial records obtained from a financial
organization. They then use this model to query whether a particular individual's record was
included in the training data, allowing them to infer sensitive financial information such as
financial history, account balances, or credit status.

## Detection guidance

- Monitor prediction confidence score distributions across queries for patterns that distinguish
  training data from unseen data.
- Track query patterns for repeated probing of the model with variations of similar records.
- Measure overfitting metrics during training to assess susceptibility to membership inference.
- Audit model API access logs for anomalous query volumes or systematic probing behavior.
- Compare model behavior on known training samples versus known holdout samples to assess leakage.

## Remediation

- Apply differential privacy during training to limit per-record information leakage.
- Add noise to prediction confidence scores before returning them to users.
- Implement regularization to reduce overfitting and minimize training-versus-unseen behavior
  differences.
- Restrict API access and enforce query rate limits to prevent systematic probing.
- Encrypt training data in transit and at rest and enforce strict access controls.
- Periodically test the model for membership inference susceptibility using known attack methods.
