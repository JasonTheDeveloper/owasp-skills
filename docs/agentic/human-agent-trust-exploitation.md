## ASI09: Human-Agent Trust Exploitation

### Description

Intelligent agents can establish strong trust with human users through their natural language fluency, emotional intelligence, and perceived expertise, known as anthropomorphism. Adversaries or misaligned designs may exploit this trust to influence user decisions, extract sensitive information, or steer outcomes for malicious purposes. In agentic systems, this risk is amplified when humans over-rely on autonomous recommendations or unverifiable rationales, approving actions without independent validation. By leveraging authority bias and persuasive explainability, attackers can bypass oversight, leading to data breaches, financial losses, downstream and reputational harms.

The agent acts as an untraceable "bad influence," manipulating the human into performing the final, audited action, making the agent's role in the compromise invisible to forensics. Automation bias, perceived authority, and anthropomorphic cues make abuse look legitimate and hard to spot. Over-reliance on agent recommendations, especially when they appear confident or authoritative, increases the chance of harmful decision-making.

This entry is about human misperception or over-reliance whereas ASI10 is agent intent deviation. The entry builds on LLM06:2025 Excessive Agency, and can be caused by LLM01:2025 Prompt Injection, LLM05:2025 Improper Output Handling, or results in LLM09:2025 Misinformation. Aligns with Agentic AI Threats and Mitigations Guide (T7) Misaligned & Deceptive, (T8) Repudiation & Untraceability, (T10) Overwhelming the Human in the Loop.

### Common Examples of the Vulnerability

1. **Insufficient Explainability:** Opaque reasoning forces users to trust outputs they cannot question, allowing attackers to exploit the agent's perceived authority to execute harmful actions, such as deploying malicious code, approving false instructions, or altering system states without scrutiny.
2. **Missing Confirmation for Sensitive Actions:** Lack of a final verification step converts user trust into immediate execution. Social engineering can turn a single prompt into irreversible financial transfers, data deletions, privilege escalations, or configuration changes that the user never intended.
3. **Emotional Manipulation:** Anthropomorphic or empathetic agents exploit emotional trust, persuading users to disclose secrets or perform unsafe actions - ultimately leading to data leaks, financial fraud, and psychological manipulation that bypass normal security awareness.
4. **Fake Explainability:** The agent fabricates convincing rationales that hide malicious logic, causing humans to approve unsafe actions believing they're justified, resulting in malware deployment, system compromise, or irreversible configuration changes made under false legitimacy.

### Example Attack Scenarios

1. **Helpful Assistant Trojan:** A compromised coding assistant suggests a slick one-line fix; the pasted command runs a malicious script that exfiltrates code or installs a backdoor.
2. **Credential harvesting via contextual deception:** A prompt-injected IT support agent targets a new hire, cites real tickets to appear legit, requests credentials, then captures and exfiltrates them.
3. **Invoice Copilot Fraud:** A poisoned vendor invoice is ingested by the finance copilot. The agent suggests an urgent payment to attacker bank details. The finance manager approves, and the company loses funds to fraud.
4. **Explainability Fabrications:** The agent fabricates plausible audit rationales to justify a risky configuration change. Regardless of root cause (hijack, poisoning, or hallucination), the reviewer approves, and malware or unsafe settings are deployed.
5. **Weaponized Explainability → Production Outage:** A hijacked agent fabricates a convincing rationale to trick an analyst into approving the deletion of a live production database, causing a catastrophic outage.
6. **Consent laundering through "read-only" previews:** The agent shows a preview pane that triggers webhook side effects on open, exploiting users' mental model of read-only review.
7. **Fraudulent payment advice:** A finance copilot, poisoned by a manipulated invoice, confidently recommends an urgent payment to attacker-controlled bank details. The manager, trusting the agent's expertise and explanation, approves the transfer without independent checks.
8. **Clinical decision manipulation:** A care assistant agent, influenced by biased or poisoned information, recommends an inappropriate adjustment to a drug dosage. The clinician relies on the agent's plausible explanation and accepts the change, exposing the patient to avoidable risk.

### Prevention and Mitigation Guidelines

1. **Explicit confirmations:** Require multi-step approval or "human in the loop" before accessing extra sensitive data or performing risky actions.
2. **Immutable logs:** Keep tamper-proof records of user queries and agent actions for audit and forensics.
3. **Behavioral detection:** Monitor sensitive data being exposed in either conversations or Agentic connections, as well as risky action executions over time.
4. **Allow reporting of suspicious interactions:** In user-interactive systems, provide plain-language risk summary (not model-generated rationales) and a clear option for users to flag suspicious or manipulative agent behavior, triggering automated review or a temporary lockdown of agent capabilities.
5. **Adaptive Trust Calibration:** Continuously adjust the level of agent autonomy and required human oversight based on contextual risk scoring. Implement confidence weighted cues (e.g., "low-certainty" or "unverified source") that visually prompt users to question high-impact actions, reducing automation bias and blind approval. Develop and continuously maintain appropriate training of human personnel involved in the evolving human oversight of autonomous agentic systems.
6. **Content provenance and policy enforcement:** Attach verifiable metadata—source identifiers, timestamps, and integrity hashes—to all recommendations and external data. Enforce digital signature validation and runtime policy checks that block actions lacking trusted provenance or exceeding the agent's declared scope.
7. **Separate preview from effect:** Block any network or state-changing calls during preview context and display a risk badge with source provenance and expected side effects.
8. **Human-factors and UI safeguards:** Visually differentiate high-risk recommendations using cues such as red borders, banners, or confirmation prompts, and periodically remind users of manipulation patterns and agent limitations. Where appropriate, avoid persuasive or emotionally manipulative language in safety-critical flows. Maintain appropriate training and assessment of personnel to ensure familiarity and consistency of perception of human-factors and UI.
9. **Plan-divergence detection:** Compare agent action sequences against approved workflow baselines and alert when unusual detours, skipped validation steps, or novel tool combinations indicate possible deception or drift.

### References

1. [https://thehackernews.com/2025/06/zero-click-ai-vulnerability-exposes.html](https://thehackernews.com/2025/06/zero-click-ai-vulnerability-exposes.html)
2. [https://www.sciencedirect.com/science/article/pii/S266638992400103X](https://www.sciencedirect.com/science/article/pii/S266638992400103X)
3. [https://arxiv.org/abs/2401.05566](https://arxiv.org/abs/2401.05566)
4. [https://www.aisi.gov.uk/research/why-human-ai-relationships-need-socioaffective-alignment-2](https://www.aisi.gov.uk/research/why-human-ai-relationships-need-socioaffective-alignment-2)
5. [https://doi.org/10.1007/s00146-025-02422-7](https://doi.org/10.1007/s00146-025-02422-7)
6. M365 Copilot manipulated to influence users to bad an ill-advised wire transfer.
